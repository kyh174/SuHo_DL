{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기\n",
    "\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient_2d(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return numerical_gradient_1d(f, x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx, a in enumerate(x):\n",
    "            grad[idx] = numerical_gradient_1d(f, a)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 구현\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient_1d(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사법으로 f(x0, x1) = x0^2 + x1^2 의 최솟값을 구하기\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 lr = 10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작은 예 lr = 1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기를 구하는 코드\n",
    "\n",
    "# 소프트맥스 함수\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x - c) # 오버플로 대책\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    y = exp_x / sum_exp_x\n",
    "\n",
    "    return y\n",
    "\n",
    "# CEE (Cross Entropy Error)\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.rand(2, 3) # 정규분포로 초기화\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63505839 0.17986436 0.89338109]\n",
      " [0.78663061 0.43114324 0.66209967]]\n",
      "\n",
      "\n",
      "[1.08900258 0.49594753 1.13191836]\n",
      "2\n",
      "\n",
      "\n",
      "0.9112431005523967\n",
      "[[ 0.23108146  0.12770402 -0.35878548]\n",
      " [ 0.34662219  0.19155603 -0.53817821]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p)) # 최댓값의 인덱스\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "t = np.array([0, 0, 1]) # 정답 레이블\n",
    "print(net.loss(x, t))\n",
    "\n",
    "# 기울기 구하기\n",
    "# def f(W):\n",
    "#     return net.loss(x, t)\n",
    "\n",
    "f = lambda W: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient_2d(f, net.W)\n",
    "print(dW)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
