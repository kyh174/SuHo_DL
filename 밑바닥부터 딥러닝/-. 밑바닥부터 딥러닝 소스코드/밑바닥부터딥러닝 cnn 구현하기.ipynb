{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd989816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ed49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.util import im2col\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97679e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)  # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5,stride = 1, pad = 0)\n",
    "print(col1.shape)  # (9, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d712526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7)  # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5)\n",
    "print(col2.shape)  # (90, 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31c7c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7)  # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5,stride =1, pad = 1)\n",
    "print(col2.shape)  # (90, 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c3a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        # 입력 데이터와 필터를 2차원 배열로 전개하고 내적한다.\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T  # 필터 전개\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        # reshape에서 -1 : 원소 개수에 맞춰 적절하게 묶어줌.\n",
    "        # transpose : 다차원 배열의 축 순서를 바꿔줌(N,H,W,C) -> (N,C,H,W)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162eb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 전개\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # 최댓값 axis : 축의 방향, 0=열방향, 1=행방향\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # 성형\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc35c20",
   "metadata": {},
   "source": [
    "CNN 구현 전체 코드 및 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3bc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"\n",
    "    다음과 같은 CNN을 구성한다.\n",
    "    → Conv → ReLU → Pooling → Affine → ReLU → Affine → Softmax →\n",
    "    전체 구현은 simple_convnet.py 참고\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        \n",
    "        # 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼내고 출력 크기를 계산한다.\n",
    "        filter_num = conv_param['filter_num'] \n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        \n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * # 풀링 계층의 출력 크기 계산\n",
    "                               (conv_output_size/2))\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "\n",
    "        # 1층의 합성곱 계층과 2, 3층의 완전연결 계층의 가중치와 편향 생성\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------   \n",
    "    \n",
    "    \n",
    "        # CNN을 구성하는 계층을 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "        \n",
    "# --------이전 수업주차에서 발표했던 내용.--------        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"추론을 수행\"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실함수 값 계산\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"오차역전파법으로 기울기를 구함\"\"\"\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "# 본 신경망으로 실제 MNIST 데이터셋을 학습하는 코드는 train_convnet.py 참고\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ffbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299183017419629\n",
      "=== epoch:1, train acc:0.2, test acc:0.18 ===\n",
      "train loss:2.2948829112850238\n",
      "train loss:2.289975877933789\n",
      "train loss:2.2857038591013468\n",
      "train loss:2.276006664744325\n",
      "train loss:2.259622244872999\n",
      "train loss:2.247924498348728\n",
      "train loss:2.232139444768667\n",
      "train loss:2.1945323744700205\n",
      "train loss:2.169498205522848\n",
      "train loss:2.1421194157498897\n",
      "train loss:2.0668302809708163\n",
      "train loss:2.0317561013534826\n",
      "train loss:1.9604189798230192\n",
      "train loss:1.9609938333999686\n",
      "train loss:1.900716757225844\n",
      "train loss:1.7474502414172748\n",
      "train loss:1.768815622988031\n",
      "train loss:1.6267020244085522\n",
      "train loss:1.625198504347529\n",
      "train loss:1.5170138918118192\n",
      "train loss:1.5648153448998905\n",
      "train loss:1.3876271932305904\n",
      "train loss:1.310168702775476\n",
      "train loss:1.2974814710106362\n",
      "train loss:1.1398656163426493\n",
      "train loss:1.1845033316875517\n",
      "train loss:0.9496771924645848\n",
      "train loss:0.8958167001353563\n",
      "train loss:0.9509507509405968\n",
      "train loss:0.8689258059700574\n",
      "train loss:0.9847312037788134\n",
      "train loss:0.7948861509644084\n",
      "train loss:0.6438343844164146\n",
      "train loss:0.6712594329058712\n",
      "train loss:0.6370709375339428\n",
      "train loss:0.8431049504572959\n",
      "train loss:0.5781664563904865\n",
      "train loss:0.6993266227222579\n",
      "train loss:0.6046036399918832\n",
      "train loss:0.621601747397977\n",
      "train loss:0.7691398582134997\n",
      "train loss:0.5044862416315395\n",
      "train loss:0.679889682142686\n",
      "train loss:0.6860235337303944\n",
      "train loss:0.5108508727093671\n",
      "train loss:0.5905784240223371\n",
      "train loss:0.4869983566013618\n",
      "train loss:0.5276759254250837\n",
      "train loss:0.5340250861802737\n",
      "train loss:0.5074894006013168\n",
      "=== epoch:2, train acc:0.815, test acc:0.799 ===\n",
      "train loss:0.5418746274534789\n",
      "train loss:0.34753166640862543\n",
      "train loss:0.45532201151605334\n",
      "train loss:0.5385215922377798\n",
      "train loss:0.5280565465877267\n",
      "train loss:0.4217847153844091\n",
      "train loss:0.4818514336724331\n",
      "train loss:0.44857273560183614\n",
      "train loss:0.24737462460964438\n",
      "train loss:0.5092783576449669\n",
      "train loss:0.408408860931603\n",
      "train loss:0.47043309450317944\n",
      "train loss:0.44248404005525155\n",
      "train loss:0.393927158452028\n",
      "train loss:0.39396992770588857\n",
      "train loss:0.39597143370463117\n",
      "train loss:0.5651945743703424\n",
      "train loss:0.30627282478990997\n",
      "train loss:0.3081172812002227\n",
      "train loss:0.3778260334113246\n",
      "train loss:0.4475711851557547\n",
      "train loss:0.2886769824332353\n",
      "train loss:0.3703598369960838\n",
      "train loss:0.3597379113601722\n",
      "train loss:0.39397048046216665\n",
      "train loss:0.30806727688893704\n",
      "train loss:0.30818321969861534\n",
      "train loss:0.3229781851242267\n",
      "train loss:0.2710796615706079\n",
      "train loss:0.3738120718003677\n",
      "train loss:0.3652096702695844\n",
      "train loss:0.36551730557281786\n",
      "train loss:0.23827664949246702\n",
      "train loss:0.27601471805702255\n",
      "train loss:0.39994641338891623\n",
      "train loss:0.4601663732347175\n",
      "train loss:0.42923972287977397\n",
      "train loss:0.27650034941713647\n",
      "train loss:0.4118961937288975\n",
      "train loss:0.2968676061798028\n",
      "train loss:0.4315229571678845\n",
      "train loss:0.29117277646576084\n",
      "train loss:0.22455731552017488\n",
      "train loss:0.41568267634482503\n",
      "train loss:0.3236376821847275\n",
      "train loss:0.27167804028248094\n",
      "train loss:0.4046539878722551\n",
      "train loss:0.34367502706393904\n",
      "train loss:0.30580817262598897\n",
      "train loss:0.41240002787655455\n",
      "=== epoch:3, train acc:0.875, test acc:0.869 ===\n",
      "train loss:0.3869554028702435\n",
      "train loss:0.270655757485797\n",
      "train loss:0.3528697549725994\n",
      "train loss:0.4793589949567166\n",
      "train loss:0.4645501729558674\n",
      "train loss:0.22894668290524214\n",
      "train loss:0.13168924107199742\n",
      "train loss:0.41530214072021066\n",
      "train loss:0.3034071163013072\n",
      "train loss:0.22106736650222258\n",
      "train loss:0.2900699029770524\n",
      "train loss:0.24031367979310958\n",
      "train loss:0.27543918309221876\n",
      "train loss:0.45041884522352743\n",
      "train loss:0.28487538108640886\n",
      "train loss:0.4373398612807229\n",
      "train loss:0.26084143923471004\n",
      "train loss:0.3925453232746989\n",
      "train loss:0.45069053472782167\n",
      "train loss:0.3670084816645601\n",
      "train loss:0.34861089082926733\n",
      "train loss:0.348594369726663\n",
      "train loss:0.3173732602631665\n",
      "train loss:0.4108518321044066\n",
      "train loss:0.32340172048126076\n",
      "train loss:0.3915963117099729\n",
      "train loss:0.33028035892701857\n",
      "train loss:0.45471677331679944\n",
      "train loss:0.27804261487220194\n",
      "train loss:0.3474102186652885\n",
      "train loss:0.2415018372976412\n",
      "train loss:0.37485298906234676\n",
      "train loss:0.3135330081793913\n",
      "train loss:0.28259965916009677\n",
      "train loss:0.39063141864605866\n",
      "train loss:0.3687617728274129\n",
      "train loss:0.40265241411289826\n",
      "train loss:0.3874204259986563\n",
      "train loss:0.21187192651674303\n",
      "train loss:0.29170370806010537\n",
      "train loss:0.25868889119501537\n",
      "train loss:0.368601568760269\n",
      "train loss:0.36382151075469127\n",
      "train loss:0.3423612778692673\n",
      "train loss:0.3370054072414517\n",
      "train loss:0.2719497611014621\n",
      "train loss:0.33066014146899925\n",
      "train loss:0.20059234285831873\n",
      "train loss:0.4023651308763137\n",
      "train loss:0.3240436981809781\n",
      "=== epoch:4, train acc:0.891, test acc:0.873 ===\n",
      "train loss:0.30299522187207034\n",
      "train loss:0.32972364585519764\n",
      "train loss:0.2505468750498159\n",
      "train loss:0.2117644941120865\n",
      "train loss:0.23317869287331752\n",
      "train loss:0.2355799095589473\n",
      "train loss:0.4791364850428007\n",
      "train loss:0.29243516380354817\n",
      "train loss:0.3222494232071351\n",
      "train loss:0.2395441225253663\n",
      "train loss:0.229109974733922\n",
      "train loss:0.33388185646455154\n",
      "train loss:0.27174339998464697\n",
      "train loss:0.4323575723179795\n",
      "train loss:0.24781671462807467\n",
      "train loss:0.2470036664998512\n",
      "train loss:0.26723909424599546\n",
      "train loss:0.31733175685892084\n",
      "train loss:0.2618484108950753\n",
      "train loss:0.16808649997245692\n",
      "train loss:0.3332722938979974\n",
      "train loss:0.10522559917046018\n",
      "train loss:0.21039856510972016\n",
      "train loss:0.23128906889521855\n",
      "train loss:0.3703621526936181\n",
      "train loss:0.22323888499366087\n",
      "train loss:0.19589323079992232\n",
      "train loss:0.31278958848155924\n",
      "train loss:0.18556704780505404\n",
      "train loss:0.4308912276045023\n",
      "train loss:0.2539268066560622\n",
      "train loss:0.34490655340698206\n",
      "train loss:0.22234461594896132\n",
      "train loss:0.21218474889552802\n",
      "train loss:0.24512814726736995\n",
      "train loss:0.3484226829959509\n",
      "train loss:0.3477945308287028\n",
      "train loss:0.35957493635125737\n",
      "train loss:0.36859265631488713\n",
      "train loss:0.1643510986127074\n",
      "train loss:0.18314529154188885\n",
      "train loss:0.4572104034368318\n",
      "train loss:0.2025179872023709\n",
      "train loss:0.2406030510914934\n",
      "train loss:0.2282792421853002\n",
      "train loss:0.34160655720660654\n",
      "train loss:0.19189993147916323\n",
      "train loss:0.3017486624808197\n",
      "train loss:0.2119176371325865\n",
      "train loss:0.21774799384947388\n",
      "=== epoch:5, train acc:0.91, test acc:0.908 ===\n",
      "train loss:0.2920375768807614\n",
      "train loss:0.31435108166268166\n",
      "train loss:0.32294209238376054\n",
      "train loss:0.22195915114595624\n",
      "train loss:0.2703032140349154\n",
      "train loss:0.19846331050394297\n",
      "train loss:0.20492381270000398\n",
      "train loss:0.13623500227682586\n",
      "train loss:0.32852870424145764\n",
      "train loss:0.2707325483786293\n",
      "train loss:0.2365281164437143\n",
      "train loss:0.18978713904412417\n",
      "train loss:0.16753732934256932\n",
      "train loss:0.12032970826228576\n",
      "train loss:0.1966682247924246\n",
      "train loss:0.246046591676388\n",
      "train loss:0.11431607278543048\n",
      "train loss:0.1811800816567543\n",
      "train loss:0.1842294977201841\n",
      "train loss:0.23742788909581644\n",
      "train loss:0.2048660171865906\n",
      "train loss:0.21694352932600938\n",
      "train loss:0.31539382567666785\n",
      "train loss:0.157823882911281\n",
      "train loss:0.2728552327493037\n",
      "train loss:0.28079459273048446\n",
      "train loss:0.30775347590312235\n",
      "train loss:0.14422772583899357\n",
      "train loss:0.1971019997296856\n",
      "train loss:0.1802147584765109\n",
      "train loss:0.1667299653570228\n",
      "train loss:0.20366650225542965\n",
      "train loss:0.18415418181023027\n",
      "train loss:0.20087970535913047\n",
      "train loss:0.20927391176433127\n",
      "train loss:0.15427361384359992\n",
      "train loss:0.2781380212699679\n",
      "train loss:0.20368159666909733\n",
      "train loss:0.24851079597129985\n",
      "train loss:0.21190254712070755\n",
      "train loss:0.23007983167577156\n",
      "train loss:0.2193738426249998\n",
      "train loss:0.16653353874933535\n",
      "train loss:0.1709633054215638\n",
      "train loss:0.22032903955352204\n",
      "train loss:0.18214583256837527\n",
      "train loss:0.09798098935805108\n",
      "train loss:0.3071542309505093\n",
      "train loss:0.16652581208532374\n",
      "train loss:0.19474673061550052\n",
      "=== epoch:6, train acc:0.919, test acc:0.912 ===\n",
      "train loss:0.11413567847075484\n",
      "train loss:0.08945799829076165\n",
      "train loss:0.2170022954033384\n",
      "train loss:0.15141591930576231\n",
      "train loss:0.19140017375538554\n",
      "train loss:0.13150882542557663\n",
      "train loss:0.10416034856004774\n",
      "train loss:0.1267239213277052\n",
      "train loss:0.16463493578099553\n",
      "train loss:0.2319429870549714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.26578576392522957\n",
      "train loss:0.16881331853826836\n",
      "train loss:0.2922989863089829\n",
      "train loss:0.11114601172169065\n",
      "train loss:0.19526737665481783\n",
      "train loss:0.10427045808981898\n",
      "train loss:0.19859301049349992\n",
      "train loss:0.3671045118534185\n",
      "train loss:0.211018088033821\n",
      "train loss:0.08913654148466556\n",
      "train loss:0.13045328364434514\n",
      "train loss:0.2082410633928147\n",
      "train loss:0.19768011972936164\n",
      "train loss:0.31615034054794877\n",
      "train loss:0.0854643110081313\n",
      "train loss:0.13995155127310674\n",
      "train loss:0.16141403852282618\n",
      "train loss:0.3889274502130185\n",
      "train loss:0.12237046991863228\n",
      "train loss:0.18272551168738524\n",
      "train loss:0.26009667653756596\n",
      "train loss:0.20464530938540457\n",
      "train loss:0.2582700160018872\n",
      "train loss:0.1963920526270998\n",
      "train loss:0.110156978210221\n",
      "train loss:0.19365041500493527\n",
      "train loss:0.10867508134457861\n",
      "train loss:0.2424258038124928\n",
      "train loss:0.1637164110296455\n",
      "train loss:0.1666655212977507\n",
      "train loss:0.09769033989745016\n",
      "train loss:0.2540365230300574\n",
      "train loss:0.1049017084757943\n",
      "train loss:0.10266487418886495\n",
      "train loss:0.11253071521386421\n",
      "train loss:0.1731112999198441\n",
      "train loss:0.295673462036472\n",
      "train loss:0.14582251435304777\n",
      "train loss:0.13598367801018799\n",
      "train loss:0.13339741989895773\n",
      "=== epoch:7, train acc:0.934, test acc:0.921 ===\n",
      "train loss:0.09775380764770801\n",
      "train loss:0.11099607615817213\n",
      "train loss:0.07226271475692636\n",
      "train loss:0.12914290373001652\n",
      "train loss:0.10733564366116966\n",
      "train loss:0.15412149834230482\n",
      "train loss:0.1989661372446726\n",
      "train loss:0.19469161875512125\n",
      "train loss:0.21937198093517313\n",
      "train loss:0.17548965338257416\n",
      "train loss:0.20200141902760937\n",
      "train loss:0.13517259206115978\n",
      "train loss:0.12089917181642953\n",
      "train loss:0.08325568198251375\n",
      "train loss:0.140742044169188\n",
      "train loss:0.15239171217643427\n",
      "train loss:0.22141626297904268\n",
      "train loss:0.0802321384437721\n",
      "train loss:0.1991187581420133\n",
      "train loss:0.10605312140871927\n",
      "train loss:0.13203760442322904\n",
      "train loss:0.07657949880606242\n",
      "train loss:0.11778420084397931\n",
      "train loss:0.1262617248629779\n",
      "train loss:0.18505077900842778\n",
      "train loss:0.15343054515560084\n",
      "train loss:0.19523901963395843\n",
      "train loss:0.06710141777178008\n",
      "train loss:0.07230676071820721\n",
      "train loss:0.11897788436664566\n",
      "train loss:0.09300791997058512\n",
      "train loss:0.22066255861315018\n",
      "train loss:0.07490566229048626\n",
      "train loss:0.10937433092666185\n",
      "train loss:0.15463687852696026\n",
      "train loss:0.11206492709555242\n",
      "train loss:0.13833272347488137\n",
      "train loss:0.1260554448744128\n",
      "train loss:0.19466581245154438\n",
      "train loss:0.0834637692858439\n",
      "train loss:0.1762457434626253\n",
      "train loss:0.23135550659771986\n",
      "train loss:0.11214898437096316\n",
      "train loss:0.17397884977725844\n",
      "train loss:0.16523224541640769\n",
      "train loss:0.06493627290455736\n",
      "train loss:0.1776988271617798\n",
      "train loss:0.13672328634872805\n",
      "train loss:0.14569291628602707\n",
      "train loss:0.08271511357809913\n",
      "=== epoch:8, train acc:0.934, test acc:0.92 ===\n",
      "train loss:0.14913950413197116\n",
      "train loss:0.3053692448206413\n",
      "train loss:0.09604420117522837\n",
      "train loss:0.18138838266551818\n",
      "train loss:0.12377955473336007\n",
      "train loss:0.049284906656743635\n",
      "train loss:0.11450948324115826\n",
      "train loss:0.22338062769835726\n",
      "train loss:0.10083571479343903\n",
      "train loss:0.23431959886322368\n",
      "train loss:0.14618706980639795\n",
      "train loss:0.16627074537551192\n",
      "train loss:0.05962754540829808\n",
      "train loss:0.12596759089049026\n",
      "train loss:0.11168203002354479\n",
      "train loss:0.10725158597932603\n",
      "train loss:0.11280781438672627\n",
      "train loss:0.06490633042952032\n",
      "train loss:0.06410315071736242\n",
      "train loss:0.10174088913836771\n",
      "train loss:0.14909973871797688\n",
      "train loss:0.13221272887194835\n",
      "train loss:0.13946881792501992\n",
      "train loss:0.06987848794573169\n",
      "train loss:0.12430714190671123\n",
      "train loss:0.11160737890478357\n",
      "train loss:0.11431738848798874\n",
      "train loss:0.1255721055146423\n",
      "train loss:0.1314574197785339\n",
      "train loss:0.11734747263155146\n",
      "train loss:0.22221978477540408\n",
      "train loss:0.06382587244595272\n",
      "train loss:0.1980318694435525\n",
      "train loss:0.11744051442158121\n",
      "train loss:0.12778854559847116\n",
      "train loss:0.053147385967441625\n",
      "train loss:0.05947402706808498\n",
      "train loss:0.1532691637961329\n",
      "train loss:0.07391193769840734\n",
      "train loss:0.1422591848757361\n",
      "train loss:0.1506064364681832\n",
      "train loss:0.06967383250782014\n",
      "train loss:0.09449323399607062\n",
      "train loss:0.10487634307234885\n",
      "train loss:0.10413025625679567\n",
      "train loss:0.07573263306121034\n",
      "train loss:0.15548672249477244\n",
      "train loss:0.034179857492784085\n",
      "train loss:0.1611170558865274\n",
      "train loss:0.1660910414109016\n",
      "=== epoch:9, train acc:0.948, test acc:0.932 ===\n",
      "train loss:0.11309266532130327\n",
      "train loss:0.1451985722685016\n",
      "train loss:0.20209255771859155\n",
      "train loss:0.09145832654050282\n",
      "train loss:0.11629207531909862\n",
      "train loss:0.11144857156053195\n",
      "train loss:0.10123601991155587\n",
      "train loss:0.05399869035676036\n",
      "train loss:0.08298550805984985\n",
      "train loss:0.16916222898714564\n",
      "train loss:0.05846172971435555\n",
      "train loss:0.0711117791277976\n",
      "train loss:0.10864068283465939\n",
      "train loss:0.06392854167147816\n",
      "train loss:0.06251366418681706\n",
      "train loss:0.04190624666762481\n",
      "train loss:0.09561338431027343\n",
      "train loss:0.12014062716693136\n",
      "train loss:0.10060103014407426\n",
      "train loss:0.13341616410592994\n",
      "train loss:0.0631583174051337\n",
      "train loss:0.15827642015422433\n",
      "train loss:0.137086475467866\n",
      "train loss:0.11135072500368182\n",
      "train loss:0.1256340085788122\n",
      "train loss:0.09511867233232976\n",
      "train loss:0.07520161521720552\n",
      "train loss:0.09265216105606598\n",
      "train loss:0.13633423044957813\n",
      "train loss:0.10768385533287912\n",
      "train loss:0.16443361930795297\n",
      "train loss:0.08340784991442751\n",
      "train loss:0.20308584687826847\n",
      "train loss:0.07901911603548156\n",
      "train loss:0.10001475787009728\n",
      "train loss:0.25371395505737704\n",
      "train loss:0.07841966104928179\n",
      "train loss:0.08545580936087446\n",
      "train loss:0.11838204137022931\n",
      "train loss:0.11086324034966859\n",
      "train loss:0.08119920488419022\n",
      "train loss:0.07621507565798863\n",
      "train loss:0.24145892586620704\n",
      "train loss:0.12436935425014634\n",
      "train loss:0.09141889174664383\n",
      "train loss:0.06937072233983226\n",
      "train loss:0.1564790348068265\n",
      "train loss:0.10099796141337385\n",
      "train loss:0.12649077609204315\n",
      "train loss:0.13891856513317422\n",
      "=== epoch:10, train acc:0.954, test acc:0.931 ===\n",
      "train loss:0.10592358078119216\n",
      "train loss:0.18811303394131584\n",
      "train loss:0.07377087879680418\n",
      "train loss:0.09006914997518922\n",
      "train loss:0.09938468988305438\n",
      "train loss:0.18381052947849771\n",
      "train loss:0.08825535255595646\n",
      "train loss:0.09146445615114976\n",
      "train loss:0.0374327042523222\n",
      "train loss:0.10623567732717479\n",
      "train loss:0.08014752098544266\n",
      "train loss:0.06220016139826759\n",
      "train loss:0.14402765773097767\n",
      "train loss:0.09457717583136917\n",
      "train loss:0.20939345493614522\n",
      "train loss:0.13154103416273083\n",
      "train loss:0.09965688919320045\n",
      "train loss:0.08380713303571036\n",
      "train loss:0.08843881741307519\n",
      "train loss:0.08493095158732389\n",
      "train loss:0.1986294210402643\n",
      "train loss:0.1936655449733904\n",
      "train loss:0.14501729280091777\n",
      "train loss:0.09860627519754425\n",
      "train loss:0.07337669692335372\n",
      "train loss:0.0758375581369843\n",
      "train loss:0.0750669161733854\n",
      "train loss:0.07001114487389021\n",
      "train loss:0.1109878065207687\n",
      "train loss:0.08615102297852538\n",
      "train loss:0.1370655381526545\n",
      "train loss:0.053661967015112444\n",
      "train loss:0.07510637691619927\n",
      "train loss:0.06202166930797814\n",
      "train loss:0.09834129842893842\n",
      "train loss:0.0744778463050222\n",
      "train loss:0.21010324372611058\n",
      "train loss:0.09829597417846615\n",
      "train loss:0.04087817244132447\n",
      "train loss:0.17589651688502148\n",
      "train loss:0.04100325815509496\n",
      "train loss:0.1329050709484685\n",
      "train loss:0.08326406048684738\n",
      "train loss:0.060542362960925494\n",
      "train loss:0.032257200935305666\n",
      "train loss:0.08509241841378384\n",
      "train loss:0.05576572195661203\n",
      "train loss:0.07095807899885123\n",
      "train loss:0.17653874188300958\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.94\n",
      "[[ -3.80449432 -11.28792437   0.16139657   4.66444541  -7.78173666\n",
      "   -3.97238007 -18.49932878  12.30895688  -4.25357965   0.87774427]]\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n=============== Final Test Accuracy ===============\\ntest acc:0.959\\n\\n전체로 학습했을 경우 약 98%까지 가능\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "pred= np.expand_dims(x_test[0], axis =0)\n",
    "pred = network.predict(pred)\n",
    "print(pred)\n",
    "print(np.argmax(pred))\n",
    "\n",
    "\"\"\"\n",
    "=============== Final Test Accuracy ===============\n",
    "test acc:0.959\n",
    "\n",
    "전체로 학습했을 경우 약 98%까지 가능\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0202d343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3905680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
